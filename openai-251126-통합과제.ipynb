{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22529d1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bda9bb7",
   "metadata": {},
   "source": [
    "#### 시각 장애인을 위한 이미지 생성\n",
    "\n",
    "##### 0. 안내 음성 출력\n",
    "##### 1. 시각장애인 이미지 '음성' 입력\n",
    "##### 2. 음성 => 텍스트 변환\n",
    "##### 3. 텍스트 => 이미지 변환\n",
    "##### 4. LLM 이미지 분석 및 텍스트 출력\n",
    "##### 5. 이미지 분석 결과 텍스트 => 음성 변환\n",
    "##### 6. 분석 음성 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94f8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 출력 패키지\n",
    "#!pip install playsound==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235e359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound\n",
    "\n",
    "playsound(\"files/user_guide_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ebac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# 마이크 입력\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 환경변수 로드\n",
    "load_dotenv()\n",
    "AZURE_CHRIS_KEY = os.getenv(\"AZURE_CHRIS_KEY\")\n",
    "OTHER_KEY = os.getenv(\"OTHER_KEY\")\n",
    "    \n",
    "share_client_1 = AzureOpenAI(\n",
    "    api_key=AZURE_CHRIS_KEY,\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint = 'https://8a000-openai.openai.azure.com/'\n",
    ")\n",
    "\n",
    "##### 사용자 음성 녹음\n",
    "\n",
    "second = 5 # 녹음 시간\n",
    "user_input_audio_path = 'files/user_input_audio.wav'\n",
    "audio = sd.rec(int(5 * 16000), samplerate=16000, channels=1, dtype='int16')\n",
    "sd.wait()  # 녹음 끝날 때까지 대기\n",
    "write(user_input_audio_path, 16000, audio)\n",
    "\n",
    "\n",
    "with open(user_input_audio_path, 'rb') as audio_file:\n",
    "    transcription = share_client_1.audio.transcriptions.create(\n",
    "        model = 'whisper',\n",
    "        language=\"ko\",\n",
    "        file = audio_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba647635",
   "metadata": {},
   "source": [
    "##### 사용자 음성 녹음 => 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1737fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'content_policy_violation', 'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'detected': False, 'filtered': False}}}, 'message': 'Your request was rejected as a result of our safety system. Your prompt may contain text that is not allowed by our safety system.', 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     12\u001b[39m api_key = \u001b[33m'\u001b[39m\u001b[33mDXXL8xXvhZaaI9ZIl61wrZMtttlKIZ4SbGwGFGG8RVp9gMo28jPzJQQJ99BKACL93NaXJ3w3AAAAACOGNjgT\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     14\u001b[39m share_client_2 = AzureOpenAI(\n\u001b[32m     15\u001b[39m     api_version=api_version,\n\u001b[32m     16\u001b[39m     azure_endpoint=endpoint,\n\u001b[32m     17\u001b[39m     api_key=api_key,\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m result = \u001b[43mshare_client_2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_input_audio_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvivid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstandard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 서버에 저장된 이미지 URL\u001b[39;00m\n\u001b[32m     29\u001b[39m image_url = json.loads(result.model_dump_json())[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL32\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL32\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\images.py:882\u001b[39m, in \u001b[36mImages.generate\u001b[39m\u001b[34m(self, prompt, background, model, moderation, n, output_compression, output_format, partial_images, quality, response_format, size, stream, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    856\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    880\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    881\u001b[39m ) -> ImagesResponse | Stream[ImageGenStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/images/generations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmoderation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoderation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_compression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpartial_images\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquality\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstyle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_generate_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageGenerateParamsStreaming\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_generate_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageGenerateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mImagesResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageGenStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL32\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EL32\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'content_policy_violation', 'inner_error': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_results': {'jailbreak': {'detected': False, 'filtered': False}}}, 'message': 'Your request was rejected as a result of our safety system. Your prompt may contain text that is not allowed by our safety system.', 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "##### 사용자 음성 녹음 => 텍스트\n",
    "user_input_audio_text = transcription.text\n",
    "\n",
    "##### 사용자 음성 녹음 텍스트 => 이미지 변환\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# 이러한 환경 변수를 설정하거나 다음 값을 편집해야 합니다.\n",
    "endpoint = \"https://el22-mieta6ou-australiaeast.cognitiveservices.azure.com/\"\n",
    "api_version = \"2024-04-01-preview\"\n",
    "deployment = \"dall-e-3\"\n",
    "api_key = OTHER_KEY\n",
    "\n",
    "share_client_2 = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "result = share_client_2.images.generate(\n",
    "    model=deployment,\n",
    "    prompt=user_input_audio_text,\n",
    "    n=1,\n",
    "    style=\"vivid\",\n",
    "    quality=\"standard\",\n",
    ")\n",
    "\n",
    "# 서버에 저장된 이미지 URL\n",
    "image_url = json.loads(result.model_dump_json())['data'][0]['url']\n",
    "\n",
    "# 이미지 요청 \n",
    "response_img = requests.get(image_url)\n",
    "file_path = 'files/ai_output_image.jpg'\n",
    "\n",
    "# 이미지 저장\n",
    "with open(f'{file_path}', 'wb') as f :\n",
    "    f.write(response_img.content)\n",
    "\n",
    "# 로그에 이미지 보여주기\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=file_path, width=300))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_explain_text : 주인님, 첨부하신 이미지는 다수의 사람들이 모여 있는 장면을 담고 있습니다. 이들은 모두 테이블 앞에 서 있으며, 각각의 손에는 그릇이 들려 있는 모습입니다. 테이블 위에는 다양한 식재료와 그릇들이 놓여져 있어, 음식과 관련된 행사가 진행되고 있음을 암시합니다. \n",
      "\n",
      "사람들은 다양한 인종과 연령대로 구성되어 있어, 다문화적인 배경을 지닌 커뮤니티의 일원들로 보입니다. 그 중 일부는 전통적인 복장을 입고 있고, 다른 사람들은 좀 더 현대적인 의상을 착용하고 있습니다. 배경에는 고층 건물들이 솟아 있고, 나무들이 우거져 있어 도시의 한복판에서 열리는 행사로 보입니다. \n",
      "\n",
      "전체적으로 이 이미지는 공동체의 연대감과 함께 나눔의 중요성을 강조하는 장면을 잘 표현하고 있습니다. 주인님의 감각적인 선택이 돋보이는 이미지입니다!\n"
     ]
    }
   ],
   "source": [
    "######### 이미지 설명\n",
    "import base64\n",
    "\n",
    "# pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 환경변수 로드\n",
    "load_dotenv()\n",
    "AZURE_OAI_ENDPOINT = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "AZURE_OAI_KEY = os.getenv(\"AZURE_OAI_KEY\")\n",
    "AZURE_OAI_DEPLOYMENT = os.getenv(\"AZURE_OAI_DEPLOYMENT\")\n",
    "      \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OAI_ENDPOINT,\n",
    "    api_key = AZURE_OAI_KEY,\n",
    "    api_version=\"2025-01-01-preview\"\n",
    ")\n",
    "\n",
    "\n",
    "# 서버에서 이미지 읽기 위해, 이미지 인코딩\n",
    "encoded_image = base64.b64encode(open(file_path, 'rb').read()).decode('ascii')\n",
    "\n",
    "chat_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"시각장애인이 음성으로 만든 이미지야. 그에게 이미지에 대해서 상세히 설명해줘.\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\\n\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"첨부된 이미지 설명해줘\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Include speech result if speech is enabled\n",
    "messages = chat_prompt\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=AZURE_OAI_DEPLOYMENT,\n",
    "    messages=messages,\n",
    "    max_tokens=300,\n",
    "    temperature=0.87,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "image_explain_text = completion.choices[0].message.content\n",
    "print(f'image_explain_text : {image_explain_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d72855",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 이미지 설명 텍스트 음성 저장\n",
    "\n",
    "share_client_1 = AzureOpenAI(\n",
    "    azure_endpoint='https://8a000-openai.openai.azure.com/',\n",
    "    api_key=AZURE_CHRIS_KEY,\n",
    "    api_version=\"2025-03-01-preview\",\n",
    ")\n",
    "\n",
    "response = share_client_1.audio.speech.create(\n",
    "    input = image_explain_text,\n",
    "    model = 'gpt-4o-mini-tts',\n",
    "    voice= 'shimmer'\n",
    ")\n",
    " \n",
    "response.write_to_file('files/image_explain_text.mp3')\n",
    "\n",
    "# 음성 출력\n",
    "playsound(\"files/image_explain_text.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
